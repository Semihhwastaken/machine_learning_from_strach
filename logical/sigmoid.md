# Lojistik Regresyon DetaylÄ± AÃ§Ä±klama

## 1. SÄ±nÄ±f YapÄ±sÄ± ve BaÅŸlangÄ±Ã§

```python
class LogisticRegression:
    def __init__(self, learning_rate=0.01, num_iterations=1000):
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        self.weights = None
        self.bias = None
```
- `learning_rate`: Bu parametre, modelin her adÄ±mda aÄŸÄ±rlÄ±klarÄ±nÄ± ne kadar deÄŸiÅŸtireceÄŸini belirler. 
Ã–ÄŸrenme hÄ±zÄ±, modelin daha iyi bir tahmin yapmayÄ± Ã¶ÄŸrenebilmesi iÃ§in temel bir parametredir.
Gradient Descent (Gradyan Ä°niÅŸi) gibi algoritmalarda, Ã¶ÄŸrenme hÄ±zÄ±, aÄŸÄ±rlÄ±klarÄ±n gÃ¼ncellenmesinde kullanÄ±lan adÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼dÃ¼r:

                    ğ‘Šyeni = ğ‘Šeski âˆ’ learning_rate Ã— gradient

- `num_iterations`: Bu parametre, modelin eÄŸitimi sÄ±rasÄ±nda kaÃ§ adÄ±m atacaÄŸÄ±nÄ± belirler. Daha fazla iterasyon, modelin daha iyi bir tahmin yapmayÄ± Ã¶ÄŸrenebilmesi iÃ§in daha iyi sonuÃ§lar verir. Ã‡ok fazla iterasyon overfittinge neden olabilir.


- `weights`: Modelin her bir Ã¶zelliÄŸe (feature) verdiÄŸi Ã¶nemi temsil eden Ã¶ÄŸrenilebilir parametrelerdir. AÄŸÄ±rlÄ±klar genellikle baÅŸlangÄ±Ã§ta sÄ±fÄ±r veya rastgele bir deÄŸerle baÅŸlatÄ±lÄ±r ve her iterasyonda Ã¶ÄŸrenme algoritmasÄ± tarafÄ±ndan gÃ¼ncellenir. Bir regresyon veya sÄ±nÄ±flandÄ±rma problemi Ã§Ã¶zÃ¼lÃ¼rken, aÄŸÄ±rlÄ±klar modelin girdilere (Ã¶rneÄŸin, bir veri kÃ¼mesindeki sÃ¼tunlar) nasÄ±l tepki vereceÄŸini belirler.


- `bias`: Modelin, girdiler sÄ±fÄ±r olduÄŸunda bile bir Ã§Ä±kÄ±ÅŸ deÄŸeri (offset) Ã¼retmesini saÄŸlayan bir Ã¶ÄŸrenilebilir parametredir. Bias, modelin veriye uyum saÄŸlayabilmesi iÃ§in dengeleme yapar.
Matematiksel olarak, bias bir sabit terimdir ve Ã§Ä±ktÄ±ya eklenir:

                        ğ‘¦tahmin = ğ‘Š â‹… ğ‘‹ + ğ‘

Burada b, girdilere baÄŸlÄ± olmayan bir parametredir. Bias, modeli daha esnek hale getirir ve yalnÄ±zca doÄŸrusal olmayan iliÅŸkileri Ã¶ÄŸrenmesini kolaylaÅŸtÄ±rmaz, aynÄ± zamanda genel doÄŸrusal iliÅŸkilerde de dengeleyici bir rol oynar.

```python
    def sigmoid(self, z):
        # TaÅŸmayÄ± Ã¶nlemek iÃ§in z deÄŸerini sÄ±nÄ±rla
        if z < -20:
            return 0
        elif z > 20:
            return 1
        try:
            return 1 / (1 + math.exp(-z))
        except OverflowError:
            return 0 if z < 0 else 1
```

Matematiksel olarak sigmoid fonksiyonu ÅŸu ÅŸekilde tanÄ±mlanÄ±r:

                        ğœ(ğ‘§) = 1 / (1 + ğ‘’^(-ğ‘§))
                        z = ğ‘Š â‹… ğ‘‹ + ğ‘
                        W = aÄŸÄ±rlÄ±klar
                        X = girdiler
                        b = bias

`z`: Modelin girdisi veya aÄŸÄ±rlÄ±klar ve bias ile hesaplanmÄ±ÅŸ bir lineer kombinasyon.
`e`: DoÄŸal logaritmanÄ±n tabanÄ± (Euler sabiti, yaklaÅŸÄ±k olarak 2.718).

1- `Ã‡Ä±kÄ±ÅŸ AralÄ±ÄŸÄ± (Range):`

Sigmoid fonksiyonu, herhangi bir z girdisini 0 ile 1 arasÄ±nda bir deÄŸere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
Bu, olasÄ±lÄ±k gibi yorumlanabilir, yani Ã§Ä±ktÄ± bir sÄ±nÄ±fa ait olma ihtimali olarak dÃ¼ÅŸÃ¼nÃ¼lebilir.

2- `Monotonluk:`

Fonksiyon her zaman artandÄ±r. Yani, z bÃ¼yÃ¼dÃ¼kÃ§e Ã§Ä±ktÄ± da bÃ¼yÃ¼r.

3- `Asimptotlar:`

zâ†’ âˆ’ âˆ olduÄŸunda ğœ(ğ‘§)â†’0.
zâ†’ + âˆ olduÄŸunda ğœ(ğ‘§)â†’1.

4- `EÅŸik DavranÄ±ÅŸÄ±:`

z=0 olduÄŸunda, sigmoid fonksiyonu tam olarak 0.5 deÄŸerini dÃ¶ndÃ¼rÃ¼r:

                        ğœ(0) = 1 / (1 + ğ‘’^0) = 0.5


```python
    def fit(self, X, y):
        # Veriyi normalize et
        X_normalized = self._normalize_features(X)
        """
        Veri setindeki her Ã¶zelliÄŸin (Ã¶rneÄŸin yaÅŸ, gelir, vb.) farklÄ± bir Ã¶lÃ§eÄŸi olabilir. Bir Ã¶zellik 0-1 arasÄ±nda deÄŸerler alÄ±rken bir diÄŸeri 0-1000 arasÄ±nda olabilir. Bu Ã¶lÃ§ek farklÄ±lÄ±klarÄ± gradyan iniÅŸinin dÃ¼zgÃ¼n Ã§alÄ±ÅŸmasÄ±nÄ± engeller. Bu yÃ¼zden tÃ¼m Ã¶zellikleri 0 ile 1 arasÄ±na Ã§ekiyoruz. Bu iÅŸleme min-max normalizasyonu denir.
        """
        
        num_samples = len(X)
        num_features = len(X[0])

        """
        num_samples: KaÃ§ tane Ã¶rnek (veri noktasÄ±) olduÄŸunu belirler.
        num_features: Her Ã¶rnekte kaÃ§ tane Ã¶zellik (baÄŸÄ±msÄ±z deÄŸiÅŸken) olduÄŸunu belirler.
        Ã–rneÄŸin:
        EÄŸer X ÅŸu ÅŸekilde bir matrisse:

        ğ‘‹ = [25, 50,
            30, 60, 
            35, 70]

        Burada num_samples = 3 (3 satÄ±r, yani 3 veri noktasÄ±)
        num_features = 2 (2 sÃ¼tun, yani 2 Ã¶zellik)
        """
        
        self.weights = [0] * num_features
        self.bias = 0

        """
        weights: Her Ã¶zelliÄŸe ait bir aÄŸÄ±rlÄ±k deÄŸeri (baÅŸlangÄ±Ã§ta sÄ±fÄ±r). Bu, modelin her Ã¶zelliÄŸin Ã§Ä±ktÄ± Ã¼zerindeki etkisini Ã¶ÄŸrenmesine yardÄ±mcÄ± olur.
        bias: Modelin sabit bir kaydÄ±rma deÄŸeri (baÅŸlangÄ±Ã§ta sÄ±fÄ±r).
        Ã–rneÄŸin: EÄŸer 2 Ã¶zellik varsa, weights = [0, 0] olur.
        """
        
        for _ in range(self.num_iterations):
            linear_pred = [sum(x_i * w_i for x_i, w_i in zip(x, self.weights)) + self.bias 
                         for x in X_normalized]
            predictions = [self.sigmoid(z) for z in linear_pred]

            """
            num_iterations: Weights and bias gÃ¼ncellenir, modelin tahminini iyileÅŸtirir.
            Bu, doÄŸrusal bir fonksiyondur:

            ğ‘§ = ğ‘¤1ğ‘¥1 + ğ‘¤2ğ‘¥2 + â‹¯ + ğ‘¤ğ‘›ğ‘¥ğ‘› + ğ‘
            x_i: EÄŸitim verisindeki bir Ã¶rneÄŸin Ã¶zellik deÄŸerleri.
            w_i: O Ã¶zelliÄŸin aÄŸÄ±rlÄ±ÄŸÄ±.
            b: Bias (modelin sabit deÄŸeri).
            Her veri noktasÄ± iÃ§in bu hesaplanÄ±r. Ã–rneÄŸin:

            EÄŸer weights = [0.5, 1], bias = 0, ve bir Ã¶rnek x = [2, 3] ise:
            ğ‘§ = 0.5 â‹… 2 + 1â‹…3 + 0 = 4

            Ã–rneÄŸin, bir z deÄŸeri 4 olduÄŸunda:

            Ïƒ(4) = 1 / (1 + e^(-4)) â‰ˆ 0.982

            Bu deÄŸer, tahmin edilen olasÄ±lÄ±ÄŸÄ± temsil eder.
            """
            
            dw = [0] * num_features
            db = 0
            
            for i in range(num_samples):
                difference = predictions[i] - y[i]
                for j in range(num_features):
                    dw[j] += X_normalized[i][j] * difference
                db += difference
            
            """
            Gradyanlar ÅŸu ÅŸekilde hesaplanÄ±r:

            âˆ‚Loss/âˆ‚wj = 1/N âˆ‘i=1N [(pi - yi) * xij]
            âˆ‚Loss/âˆ‚b = 1/N âˆ‘i=1N (pi - yi)

            """
            # Gradient descent gÃ¼ncelleme
            for j in range(num_features):
                self.weights[j] -= self.learning_rate * (dw[j] / num_samples)
            self.bias -= self.learning_rate * (db / num_samples)
            """
            wj = wj - Î± * âˆ‚wj / âˆ‚Loss -> gradient descent -> a = learning_rate
            b = b - Î± * âˆ‚b / âˆ‚Loss
            """
```

```python
    def compute_cost(self, X, y, predictions):
        """
        Binary Cross-Entropy Loss hesapla:
        J = -1/m * Î£(y*log(h) + (1-y)*log(1-h))
        """
        m = len(y)
        cost = 0
        epsilon = 1e-15  # log(0) hatasÄ± almamak iÃ§in kÃ¼Ã§Ã¼k bir deÄŸer
        
        for i in range(m):
            # predictions'Ä± 0 ve 1'e Ã§ok yakÄ±n deÄŸerlerden kaÃ§Ä±nmak iÃ§in dÃ¼zenle
            pred = max(min(predictions[i], 1 - epsilon), epsilon)
            if y[i] == 1:
                cost += -math.log(pred)
            else:
                cost += -math.log(1 - pred)
        
        return cost / m
```

`Neden Binary Cross-Entropy KullanÄ±yoruz?`
Bu maliyet fonksiyonu, lojistik regresyonun olasÄ±lÄ±k temelli doÄŸasÄ±na uygundur:

OlasÄ±lÄ±k HesaplarÄ±: Sigmoid fonksiyonunun Ã§Ä±ktÄ±larÄ±nÄ± doÄŸrudan olasÄ±lÄ±k olarak yorumlamamÄ±zÄ± saÄŸlar.
KayÄ±p HesaplarÄ±: Modelin 1 veya 0 sÄ±nÄ±fÄ± iÃ§in doÄŸru tahmin yapma becerisini Ã¶lÃ§er.
AyrÄ±ca:

DoÄŸrusal regresyondaki gibi kare farklar (MSE) yerine bu yÃ¶ntemi kullanÄ±rÄ±z Ã§Ã¼nkÃ¼:
OlasÄ±lÄ±klar (0-1 aralÄ±ÄŸÄ±nda) iÃ§in kare farklar daha az uygun bir hata metriÄŸidir.
Cross-Entropy, tahminlerin olasÄ±lÄ±k olmasÄ±nÄ± daha doÄŸru Ã¶dÃ¼llendirir.







