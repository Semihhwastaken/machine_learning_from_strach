# 1. Feature Selection Nedir?
Feature selection, veri setindeki Ã¶zelliklerin bir alt kÃ¼mesini seÃ§erek daha iyi bir performans veya daha hÄ±zlÄ± bir model oluÅŸturmayÄ± hedefler. Ä°deal bir Ã¶zellik seÃ§imi:
    AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi azaltÄ±r.
    Hesaplama maliyetini dÃ¼ÅŸÃ¼rÃ¼r.
    Modelin genelleme yeteneÄŸini artÄ±rÄ±r.
    Feature selection yÃ¶ntemleri Ã¼Ã§ ana kategoriye ayrÄ±lÄ±r:
        Filter Methods: Ä°statistiksel Ã¶lÃ§Ã¼tlere dayanÄ±r. Ã–rneÄŸin, korelasyon katsayÄ±sÄ± veya chi-square testi.
        Wrapper Methods: Ã–zellik alt kÃ¼meleri deneyerek en iyi kombinasyonu arar.
        Embedded Methods: Ã–zellik seÃ§imini model eÄŸitimine entegre eder. Ã–rneÄŸin, Lasso Regression.

# 2. Brute Force AlgoritmasÄ±
Brute Force, tÃ¼m olasÄ± Ã¶zellik kombinasyonlarÄ±nÄ± test ederek en iyi kombinasyonu seÃ§meyi hedefler. Bu yÃ¶ntem, basit ama pahalÄ± bir yÃ¶ntemdir, Ã§Ã¼nkÃ¼ Ã¶zellik sayÄ±sÄ± arttÄ±kÃ§a kombinasyon sayÄ±sÄ± Ã¼stel olarak artar.

`NasÄ±l Ã‡alÄ±ÅŸÄ±r?`
```	
1- TÃ¼m olasÄ± Ã¶zellik kombinasyonlarÄ± oluÅŸturulur.
2- Her kombinasyon iÃ§in bir model eÄŸitilir ve bir metrik (Ã¶rneÄŸin doÄŸruluk, F1 skoru) hesaplanÄ±r.
3- En iyi metriÄŸe sahip olan kombinasyon seÃ§ilir.


AvantajlarÄ±:

- En iyi Ã§Ã¶zÃ¼mÃ¼ garanti eder (optimal).
- Ã–zelliklerin model Ã¼zerindeki etkisini net bir ÅŸekilde gÃ¶rebiliriz.

DezavantajlarÄ±:

- Hesaplama maliyeti Ã§ok yÃ¼ksektir. EÄŸer ğ‘› Ã¶zellik varsa, 
- 2^nâˆ’1 kombinasyon test edilmelidir.
- BÃ¼yÃ¼k veri setleri iÃ§in pratik deÄŸildir.
```
# 3. Griddy AlgoritmasÄ± (Greedy Search)
Griddy AlgoritmasÄ±, brute force'un aksine, daha hÄ±zlÄ± bir ÅŸekilde "yeterince iyi" Ã§Ã¶zÃ¼mler bulmayÄ± hedefler. Bu algoritma, her adÄ±mda en iyi yerel seÃ§imi yapar, yani mevcut durumdaki en iyi Ã¶zelliÄŸi seÃ§er.

`NasÄ±l Ã‡alÄ±ÅŸÄ±r?`
```	
1- BoÅŸ bir Ã¶zellik kÃ¼mesiyle baÅŸlanÄ±r.
2- Her adÄ±mda, eklenmesi model performansÄ±nÄ± en Ã§ok artÄ±ran Ã¶zelliÄŸi ekler.
3- Belirli bir durdurma kriterine ulaÅŸÄ±ldÄ±ÄŸÄ±nda (Ã¶rneÄŸin, model performansÄ±nda iyileÅŸme olmamasÄ±) sÃ¼reÃ§ sona erer.


`AvantajlarÄ±:`

- Daha hÄ±zlÄ±dÄ±r Ã§Ã¼nkÃ¼ tÃ¼m kombinasyonlarÄ± test etmez.
- Ã–zellikle bÃ¼yÃ¼k veri setlerinde kullanÄ±labilir.

`DezavantajlarÄ±:`

- Optimal Ã§Ã¶zÃ¼mÃ¼ garanti etmez (yerel maksimuma takÄ±labilir).
- Performans metriklerinin seÃ§im sÄ±rasÄ±na baÄŸÄ±mlÄ±lÄ±ÄŸÄ± vardÄ±r.
```
# 4. Regularization ile Ã–zellik SeÃ§imi
DÃ¼zenlileÅŸtirme, Ã¶zellik seÃ§imi iÃ§in kullanÄ±labilir ve bir modelin karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± kontrol ederek daha iyi genelleÅŸtirme performansÄ± saÄŸlar. Brute force veya griddy algoritmalar gibi tÃ¼m olasÄ± kombinasyonlarÄ± aramak yerine, dÃ¼zenlileÅŸtirme yÃ¶ntemleri bir ceza terimi ekleyerek gereksiz Ã¶zelliklerin etkisini azaltÄ±r veya tamamen sÄ±fÄ±ra indirir.

## `L1 DÃ¼zenlileÅŸtirme (Lasso Regression)`
    L1 dÃ¼zenlileÅŸtirme, kayÄ±p fonksiyonuna katsayÄ±larÄ±n mutlak deÄŸerlerinin toplamÄ±nÄ± bir ceza olarak ekler:

$$\text{KayÄ±p Fonksiyonu} = \text{Hata} + \lambda \sum_{j=1}^n |w_j|$$

    ### NasÄ±l Ã‡alÄ±ÅŸÄ±r?
   
    - L1 dÃ¼zenlileÅŸtirme, bazÄ± katsayÄ±larÄ± sÄ±fÄ±ra indirir. BÃ¶ylece model, sÄ±fÄ±r olan Ã¶zellikleri tamamen gÃ¶z ardÄ± eder.
    - SÄ±fÄ±r olmayan katsayÄ±ya sahip Ã¶zellikler seÃ§ilmiÅŸ olur.
    - Lasso (L1 Regularization) iÃ§in alpha parametresi, dÃ¼zenlileÅŸtirme gÃ¼cÃ¼nÃ¼ kontrol eden bir hiperparametredir. Bu parametre, modelin ne kadar sÄ±kÄ± dÃ¼zenlileÅŸtirme uygulayacaÄŸÄ±nÄ± belirler ve bu durum, seÃ§ilen Ã¶zelliklerin sayÄ±sÄ±nÄ± ve modelin davranÄ±ÅŸÄ±nÄ± doÄŸrudan etkiler.
        1. KÃ¼Ã§Ã¼k Alpha DeÄŸerleri:
            - DÃ¼zenlileÅŸtirme GÃ¼cÃ¼ DÃ¼ÅŸÃ¼k: KÃ¼Ã§Ã¼k bir alpha deÄŸeri, dÃ¼zenlileÅŸtirme cezasÄ±nÄ± azaltÄ±r ve model daha fazla esneklik kazanÄ±r.
            - SonuÃ§: Daha fazla Ã¶zellik seÃ§ilir (daha az Ã¶zellik elenir). Modelin genelleÅŸtirme gÃ¼cÃ¼ azalabilir ve aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting) riski artar.
        2. BÃ¼yÃ¼k Alpha DeÄŸerleri:
            - DÃ¼zenlileÅŸtirme GÃ¼cÃ¼ YÃ¼ksek: BÃ¼yÃ¼k bir alpha deÄŸeri, dÃ¼zenlileÅŸtirme cezasÄ±nÄ± artÄ±rÄ±r ve model, gereksiz Ã¶zelliklerin katsayÄ±larÄ±nÄ± sÄ±fÄ±ra Ã§ekmeye zorlanÄ±r.
            - SonuÃ§: Daha az Ã¶zellik seÃ§ilir (daha fazla Ã¶zellik elenir). Model daha sade hale gelir, ancak aÅŸÄ±rÄ± dÃ¼zenlileÅŸtirme (underfitting) riski artabilir.
        3. Alpha = 0:
            - HiÃ§ DÃ¼zenlileÅŸtirme Yok: Bu durumda model sadece klasik lineer regresyon gibi davranÄ±r ve hiÃ§bir Ã¶zellik elenmez. 
            - SonuÃ§: TÃ¼m Ã¶zellikler modelde kalÄ±r. DÃ¼zenlileÅŸtirme olmadan, modelin aÅŸÄ±rÄ± Ã¶ÄŸrenme riski artar.
    

        AvantajlarÄ±:

            - Ã–zellik seÃ§imi doÄŸrudan yapÄ±lÄ±r.
            - YÃ¼ksek boyutlu veri kÃ¼meleri iÃ§in oldukÃ§a etkili.

        DezavantajlarÄ±:

            - Birbirine Ã§ok benzeyen (yÃ¼ksek korelasyona sahip) Ã¶zellikler varsa, bunlardan sadece birini seÃ§er.

```python
class LassoRegression:
    def __init__(self, learning_rate=0.01, lambda_param=1.0, n_iterations=1000):
        self.learning_rate = learning_rate
        self.lambda_param = lambda_param
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
```
```
- learning_rate: Gradient descent'in adÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼dÃ¼r.
- lambda_param: Regularization gÃ¼cÃ¼nÃ¼ kontrol eden hiperparametredir.
- n_iterations: Gradient descent'in kaÃ§ adÄ±mda tamamlanacaÄŸÄ±nÄ± belirler.
- weights: AÄŸÄ±rlÄ±klarÄ±n baÅŸlangÄ±Ã§ deÄŸerlerini iÃ§erir.
- bias: Bias'Ä±n baÅŸlangÄ±Ã§ deÄŸerini iÃ§erir.
```

```python
def fit(self, X, y):
        n_samples, n_features = len(X), len(X[0])

        """
        X: EÄŸitim verisi, her bir satÄ±r bir Ã¶rneÄŸi (veri noktasÄ±nÄ±), her bir sÃ¼tun ise bir Ã¶zelliÄŸi temsil eder.
        n_samples: Veri setindeki toplam Ã¶rnek sayÄ±sÄ±nÄ± verir.
        n_features: Veri setindeki toplam Ã¶zellik (feature) sayÄ±sÄ±nÄ± verir.
        """
        self.weights = [0] * n_features
        self.bias = 0

        """
        self.weights: Her Ã¶zellik iÃ§in Ã¶ÄŸrenilecek aÄŸÄ±rlÄ±klarÄ± temsil eder. BaÅŸlangÄ±Ã§ta sÄ±fÄ±r olarak ayarlanÄ±r.
        self.bias: Modelin sabit terimidir (yani tahminlerde tÃ¼m verilere eklenir). BaÅŸlangÄ±Ã§ta sÄ±fÄ±r olarak ayarlanÄ±r.
        """
        
        for _ in range(self.n_iterations):
            """iterasyon sayÄ±sÄ± kadar dÃ¶ngÃ¼"""
            y_pred = self._predict(X)
            """
            _predict metodu, mevcut aÄŸÄ±rlÄ±klar ve bias kullanÄ±larak tahmin edilen deÄŸerleri dÃ¶ndÃ¼rÃ¼r:
            ğ‘¦_i = ğ‘¤ * ğ‘‹_i + ğ‘
            
            Bu tahminler, mevcut model parametrelerinin (weights ve bias) ne kadar doÄŸru olduÄŸunu anlamak iÃ§in kullanÄ±lÄ±r.
            """
            
            dw = [0] * n_features
            db = 0
            """
            dw: Her bir aÄŸÄ±rlÄ±k (ğ‘¤_j) iÃ§in gradyanÄ± (tÃ¼revini) tutar. Lasso dÃ¼zenlileÅŸtirme iÃ§in gÃ¼ncelleme burada yapÄ±lÄ±r.
            db: Bias (ğ‘) iÃ§in gradyanÄ± tutar. 
            Gradyanlar, kayÄ±p fonksiyonunun tÃ¼revlerini temsil eder ve aÄŸÄ±rlÄ±klarÄ±n hangi yÃ¶nde gÃ¼ncellenmesi gerektiÄŸini belirtir.
            """
            
            for i in range(n_samples):
                error = y_pred[i] - y[i]
                
                db += error
                
                
                for j in range(n_features):
                    dw[j] += error * X[i][j]
                """
                error: Tahmin (ğ‘¦_pred) ile gerÃ§ek deÄŸer (ğ‘¦) arasÄ±ndaki fark.
                Bias gradyanÄ± (ğ‘‘ğ‘): TÃ¼m Ã¶rneklerin hata deÄŸerlerinin toplamÄ±dÄ±r.
                AÄŸÄ±rlÄ±k gradyanÄ± (ğ‘‘ğ‘¤[ğ‘—]): Her Ã¶zellik iÃ§in hata (ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ) ve Ã¶zellik deÄŸeri 
                (ğ‘‹[ğ‘–][ğ‘—]) Ã§arpÄ±larak hesaplanÄ±r. Bu, her aÄŸÄ±rlÄ±ÄŸÄ±n gÃ¼ncellenmesine katkÄ±da bulunur.
                """
            
            
            db /= n_samples
            for j in range(n_features):
                dw[j] = dw[j] / n_samples + self.lambda_param * self._sign(self.weights[j])
                """
                Bias ve aÄŸÄ±rlÄ±k gradyanlarÄ±, veri sayÄ±sÄ±na (ğ‘›_samples) bÃ¶lÃ¼nerek ortalamasÄ± alÄ±nÄ±r.
                Lasso dÃ¼zenlileÅŸtirme etkisi, self.lambda_param * self._sign(self.weights[j]) ile eklenir:
                self._sign(self.weights[j]): AÄŸÄ±rlÄ±ÄŸÄ±n iÅŸaretini verir (+1 veya -1). L1 dÃ¼zenlileÅŸtirme, aÄŸÄ±rlÄ±klarÄ± sÄ±fÄ±ra Ã§ekme eÄŸilimindedir.
                """
            
            # AÄŸÄ±rlÄ±klarÄ± gÃ¼ncelle
            self.bias -= self.learning_rate * db
            for j in range(n_features):
                self.weights[j] -= self.learning_rate * dw[j]
                """
                Gradyan Ä°niÅŸ GÃ¼ncellemesi:
                ğ‘ â† ğ‘ âˆ’ ğœ‚*ğ‘‘ğ‘
                ğ‘¤_ ğ‘—â†ğ‘¤_ğ‘—âˆ’ğœ‚*ğ‘‘ğ‘¤_ğ‘—
                Burada:
                Î·: Ã–ÄŸrenme oranÄ± (learning rate). GÃ¼ncelleme adÄ±mÄ±nÄ±n bÃ¼yÃ¼klÃ¼ÄŸÃ¼nÃ¼ belirler.
                Bias ve aÄŸÄ±rlÄ±klar, gradyanlarÄ±n ters yÃ¶nÃ¼nde gÃ¼ncellenir. Lasso etkisi sayesinde bazÄ± aÄŸÄ±rlÄ±klar sÄ±fÄ±ra yakÄ±nlaÅŸÄ±r veya sÄ±fÄ±r olur.
                """
```
```python
def _predict(self, X):
        predictions = []
        for xi in X:
            pred = self.bias
            for j in range(len(self.weights)):
                pred += self.weights[j] * xi[j]
            predictions.append(pred)
        return predictions
    
    def predict(self, X):
        return self._predict(X)
    
    def _sign(self, x):
        if x > 0:
            return 1
        elif x < 0:
            return -1
        return 0
```

# _predict: AmacÄ±: Modelin tahminler Ã¼retmesi.
    X: Girdi verisi (Ã¶rneklerin ve Ã¶zelliklerin bulunduÄŸu matris).
    predictions: Tahmin edilen deÄŸerlerin (ğ‘¦^) tutulduÄŸu liste.
    AdÄ±mlar:
        - for xi in X: Girdideki her Ã¶rneÄŸi (ğ‘¥_ğ‘–) sÄ±rayla iÅŸler.
        - BaÅŸlangÄ±Ã§ Tahmini: pred = self.bias
            - Model tahminine sabit bir baÅŸlangÄ±Ã§ deÄŸeri (ğ‘) eklenir.
        - AÄŸÄ±rlÄ±k ve Ã–zellik Ã‡arpÄ±mÄ±:
            - Her Ã¶zellik iÃ§in, aÄŸÄ±rlÄ±k (ğ‘¤_ğ‘—) ile Ã¶zellik deÄŸeri (ğ‘¥_ğ‘–ğ‘—) Ã§arpÄ±lÄ±r ve tahmine eklenir:
$$\text{pred} \leftarrow \text{pred} + w_j \cdot x_{ij}$$
        - Tahmin Listesine Ekleme:
            - Her bir Ã¶rnek iÃ§in hesaplanan tahmin, predictions listesine eklenir.
    SonuÃ§:
        - TÃ¼m Ã¶rnekler iÃ§in tahminler dÃ¶ndÃ¼rÃ¼lÃ¼r.


# sign:
    - AmacÄ±: SayÄ±nÄ±n iÅŸaretini belirler. Bu, L1 dÃ¼zenlileÅŸtirme sÄ±rasÄ±nda kullanÄ±lÄ±r.
    - Girdi: x (bir sayÄ±).
    - Ã‡Ä±kÄ±ÅŸ:
        1 -> EÄŸer  ğ‘¥>0
        -1 -> EÄŸer ğ‘¥<0
        0 -> EÄŸer ğ‘¥=0

    - `Neden Ä°ÅŸaret Fonksiyonu?`
            - Lasso regresyonda ğ¿1 dÃ¼zenlileÅŸtirme terimi, aÄŸÄ±rlÄ±klarÄ± sÄ±fÄ±ra Ã§ekmek iÃ§in kullanÄ±lÄ±r. Bu, aÄŸÄ±rlÄ±klarÄ±n tÃ¼revine iÅŸaret fonksiyonunun eklenmesini gerektirir:
$$\frac{\partial}{\partial w_j} \lambda |w_j| = \lambda \cdot \text{sign}(w_j)$$


# Ek Matematiksel Bilgiler

    Gradyanlar, kayÄ±p fonksiyonunun tÃ¼revleri kullanÄ±larak hesaplanÄ±r. KayÄ±p fonksiyonu Lasso Regresyonda ÅŸu ÅŸekildedir:

$$L(w,b) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^n |w_j|$$
		
    HatalarÄ±n ortalama karesel deÄŸeri (MSE - Mean Squared Error):
$$\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$$ 

    L1 dÃ¼zenlileÅŸtirme terimi, aÄŸÄ±rlÄ±klarÄ±n cezalandÄ±rÄ±lmasÄ±nÄ± saÄŸlar:
$$\lambda \sum_{j=1}^n |w_j|$$
    




	
